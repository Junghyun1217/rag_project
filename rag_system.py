import os
os.environ["HF_HOME"] = "./hf_models"
os.environ["HF_DATASETS_CACHE"] = "./hf_datasets"
print("--- 1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ ---") # <<< ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

import json
import pandas as pd
import re
import numpy as np  
import torch
import faiss
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoTokenizer

# 1. ë°ì´í„° ë¡œë”©
# ğŸš¨ ìˆ˜ì •: í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ëª… 'documents.json'ì— ë§ê²Œ ìˆ˜ì •
data_file = "documents.json"
with open(data_file, 'r', encoding='utf-8') as f:
    # ğŸš¨ product_data êµ¬ì¡°ê°€ [{'title': ..., 'text': ...}] ì´ë¯€ë¡œ, 
    # ì‹¤ìŠµ ì½”ë“œì˜ product/review í‚¤ì— ë§ê²Œ ìˆ˜ì •í•˜ì—¬ ë¡œë”©í•©ë‹ˆë‹¤.
    loaded_data = json.load(f)
    product_data = []
    for item in loaded_data:
        # í¬ë¡¤ë§ ê²°ê³¼ í‚¤: 'title' -> 'product', 'text' -> 'review'ë¡œ ë³€ê²½í•˜ì—¬ ì €ì¥
        product_data.append({'product': item['title'], 'review': item['text']})


print(f"ë¶ˆëŸ¬ì˜¨ ë°ì´í„° ê°œìˆ˜: {len(product_data)}")

# 2. í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (ë™ì¼)
def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'^ì˜ê²¬', '', text)
    text = re.sub(r'^í›„ê¸°', '', text)
    text = re.sub(r'[\t\r\n]+', ' ', text)
    # text = re.sub(r'[!@#$%^&*(),\"\']', '', text) # ë¬¸ì¥ë¶€í˜¸ ìœ ì§€í•˜ì—¬ ë¶„ì„ ì •í™•ë„ ë†’ì„
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# 3. DataFrame ìƒì„± ë° ì „ì²˜ë¦¬ (ë™ì¼)
df = pd.DataFrame(product_data)  # [{'product': ..., 'review': ...}]
df.fillna("", inplace=True)
df['clean_review'] = df['review'].apply(clean_text)
df.drop('review', axis=1, inplace=True)


# 4. BM25 ëª¨ë¸ êµ¬ì¶• (ë™ì¼)
tokenized_corpus = [doc.split() for doc in df['clean_review']]
bm25 = BM25Okapi(tokenized_corpus)

def bm25_search(query, top_n=5):
    query_tokens = query.split()
    scores = bm25.get_scores(query_tokens)
    sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
    results = []
    for idx in sorted_indices[:top_n]:
        results.append({
            'index': idx,
            'product': df.iloc[idx]['product'],
            'clean_review': df.iloc[idx]['clean_review'],
            'score': scores[idx]
        })
    return results

# 5. Sentence-BERT ì„ë² ë”©
# ğŸš¨ ì£¼ì˜: ì´ ë‹¨ê³„ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë©°, CPU/RAM ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤.
model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
sbert = SentenceTransformer(model_name)

corpus_embeddings = sbert.encode(df['clean_review'].tolist(), convert_to_tensor=True)
corpus_embeddings = corpus_embeddings.cpu().detach().numpy().astype('float32')

# 6. FAISS ì¸ë±ìŠ¤ ìƒì„±
emb_dim = corpus_embeddings.shape[1]
# ğŸš¨ CPU í™˜ê²½ì´ë¼ë©´ IndexFlatIP ëŒ€ì‹  IndexFlatL2ë¥¼ ê³ ë ¤
index = faiss.IndexFlatIP(emb_dim) 
index.add(corpus_embeddings)
print(f"FAISSì— {index.ntotal}ê°œì˜ ë²¡í„°ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 7. Dense Search í•¨ìˆ˜ (ë™ì¼)
def dense_search(query, top_n=5):
    query_emb = sbert.encode([query], convert_to_tensor=False).astype('float32')
    D, I = index.search(query_emb, top_n)
    results = []
    for idx, score in zip(I[0], D[0]):
        results.append({
            'index': idx,
            'product': df.iloc[idx]['product'],
            'clean_review': df.iloc[idx]['clean_review'],
            'score': float(score)
        })
    return results

# 8. LLM ëª¨ë¸ ë¡œë”© ë° RAG ì‹¤í–‰
# ğŸš¨ LLM ë¡œë”© ì „ GPU ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.
# ğŸš¨ Qwen/Qwen3-4B ëª¨ë¸ì€ GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤.
model_name_or_path = "Qwen/Qwen3-4B"

print(f"\n>> LLM ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name_or_path}")
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,     # bfloat16 ì‚¬ìš©
    device_map="auto",              # GPU ìë™ í• ë‹¹
    trust_remote_code=True
)
print(">> LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)


def generate_answer_with_qwen(context_docs, user_question):
    # ë¬¸ë§¥ êµ¬ì„±
    context_str = "\n\n".join([
        f"[{i+1}] ì œí’ˆëª…: {doc['product']}\në¦¬ë·°: {doc['clean_review']}"
        for i, doc in enumerate(context_docs)
    ])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Qwen ì±„íŒ… í¬ë§·)
    system_prompt = "ë‹¹ì‹ ì€ ì œí’ˆ ë¦¬ë·° ë¶„ì„ ë° ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
    user_prompt = (
        f"{context_str}\n\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸: {user_question}\n"
        "ì¶”ì²œ ì œí’ˆê³¼ ê·¸ ì´ìœ ë¥¼ í•œê¸€ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„œìˆ í•´ì£¼ì„¸ìš”."
    )

    prompt = (
        f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        f"<|im_start|>user\n{user_prompt}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    # Tokenize with attention mask
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)

    # Generate
    output = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        temperature=0.7,
        top_p=0.95,
        do_sample=True,
        max_new_tokens=768,      # ë” ì—¬ìœ  ìˆê²Œ ì¦ê°€
        repetition_penalty=1.1
    )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    if "<|im_start|>assistant" in decoded:
        answer = decoded.split("<|im_start|>assistant")[-1].strip()
    else:
        answer = decoded.strip()
        
    # Qwen ëª¨ë¸ì˜ Think í† í° ì²˜ë¦¬
    if "</think>" in answer:
        answer = answer.split("</think>")[-1].strip()
    elif "<think>" in answer:
        answer = answer.split("<think>")[-1].strip()

    return answer


# 9. RAG ì‹¤í–‰
user_question = "ë°°í„°ë¦¬ ì˜¤ë˜ê°€ëŠ” ë…¸íŠ¸ë¶ ì¶”ì²œí•´ì¤˜" # ì‹¤ì œ ì§ˆë¬¸
query = "ë°°í„°ë¦¬" # ê²€ìƒ‰ìš© í‚¤ì›Œë“œ/ì§ˆë¬¸

print(f"\nğŸ” [Dense Search ì‹¤í–‰] - ê²€ìƒ‰ì–´: '{query}'")
dense_results = dense_search(query)

print("\n--- ê²€ìƒ‰ëœ ë¬¸ë§¥(Context) ---")
for i, r in enumerate(dense_results):
    print(f"[{i+1}] ì œí’ˆ: {r['product']} (Score: {r['score']:.4f})")
    print(f"   ë¦¬ë·°: {r['clean_review'][:50]}...\n")


print(f"\nğŸ§  [LLM ë‹µë³€ ìƒì„± ì‹œì‘] - ì§ˆë¬¸: '{user_question}'")
answer = generate_answer_with_qwen(dense_results, user_question)
print("\n===============================")
print("[LLM ë‹µë³€]")
print(answer)
print("===============================")